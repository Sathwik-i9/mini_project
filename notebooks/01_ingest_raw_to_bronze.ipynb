{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa5a991-78e6-4b68-9e4a-c3d990ea1a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook\n",
    "# ---------------------------------------------------------\n",
    "# Notebook 01: Ingest Raw Source Files into Bronze Layer\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# -----------------------------\n",
    "# 1. File Paths\n",
    "# -----------------------------\n",
    "patient_src_path = \"dbfs:/FileStore/Mini_Project/Source_data/Patient_Source.csv\"\n",
    "insurance_src_path = \"dbfs:/FileStore/Mini_Project/Source_data/Insurance_Source.csv\"\n",
    "\n",
    "bronze_patient_path = \"dbfs:/FileStore/Mini_Project/bronze/patient_bronze\"\n",
    "bronze_insurance_path = \"dbfs:/FileStore/Mini_Project/bronze/insurance_bronze\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132f69c2-8198-4e66-8352-4794aa47e507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. Read Raw Patient File\n",
    "# -----------------------------\n",
    "df_patient_raw = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(patient_src_path)\n",
    "         .withColumn(\"ingest_time\", current_timestamp())\n",
    "         .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "print(\"Patient_RAW Schema:\")\n",
    "df_patient_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c0eb88-9bca-46ec-9f8c-f48f093bd55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "def clean_col(name: str) -> str:\n",
    "    # strip spaces, lower-case, replace bad chars with underscore\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"[ ,;{}()\\n\\t=]\", \"_\", name)\n",
    "    return name.lower()\n",
    "\n",
    "# After reading raw df\n",
    "df_patient_raw = spark.read.option(\"header\", True).csv(patient_src_path)\n",
    "\n",
    "# Clean column names\n",
    "df_patient_raw = df_patient_raw.toDF(*[clean_col(c) for c in df_patient_raw.columns])\n",
    "\n",
    "# Do the same for insurance\n",
    "df_insurance_raw = spark.read.option(\"header\", True).csv(insurance_src_path)\n",
    "df_insurance_raw = df_insurance_raw.toDF(*[clean_col(c) for c in df_insurance_raw.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296bf828-77f7-4892-a86f-12296a5c1ec5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS patient_bronze;\n",
    "DROP TABLE IF EXISTS insurance_bronze;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987f7868-894e-485c-b646-c447b5601057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/Mini_Project/bronze/patient_bronze\", recurse=True)\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/Mini_Project/bronze/insurance_bronze\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d8f3412-e388-4ff4-9521-37d7447e7fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. Write Patient Data to Bronze (Delta)\n",
    "# -----------------------------\n",
    "df_patient_raw.write.mode(\"overwrite\").format(\"delta\").save(bronze_patient_path)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS patient_bronze\n",
    "    USING DELTA\n",
    "    LOCATION 'dbfs:/FileStore/Mini_Project/bronze/patient_bronze'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Patient Bronze Table Created Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987163fb-54bc-403e-8b37-f919bcdd0d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4. Read Raw Insurance File\n",
    "# -----------------------------\n",
    "df_insurance_raw = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(insurance_src_path)\n",
    "         .withColumn(\"ingest_time\", current_timestamp())\n",
    "         .withColumn(\"source_file\", input_file_name())\n",
    ")\n",
    "\n",
    "print(\"Insurance_RAW Schema:\")\n",
    "df_insurance_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c851b8-779b-44d7-a105-0e803c41c90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5. Write Insurance Data to Bronze (Delta)\n",
    "# -----------------------------\n",
    "df_insurance_raw.write.mode(\"overwrite\").format(\"delta\").save(bronze_insurance_path)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS insurance_bronze\n",
    "    USING DELTA\n",
    "    LOCATION 'dbfs:/FileStore/Mini_Project/bronze/insurance_bronze'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Insurance Bronze Table Created Successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3143cd-19c4-47d9-a785-da674230f643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from patient_bronze\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10661196-414d-4f09-932a-9d5ab95a4173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select * from insurance_bronze\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3428723-9b48-4ce0-a269-de1759d23853",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_insurance_raw.printSchema()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7665633639427442,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_raw_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
