{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b566a97-7602-4efd-9718-4870aa165ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PatientInsuranceETL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71fdb944-fbf1-4f6f-bb56-52072ecd6009",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, split, col, sha2, concat, current_timestamp, upper, regexp_replace, lit\n",
    "\n",
    "# Load raw CSV\n",
    "patient_raw = spark.read.csv(\"dbfs:/FileStore/Mini_Project/Source_data/Patient_Source.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Clean and transform\n",
    "patient_landing = (\n",
    "    patient_raw\n",
    "    .withColumn(\"patient_id\", trim(col(\"patient_id\")))\n",
    "    .withColumn(\"policy_id\", trim(col(\"policy_id\")))\n",
    "    .withColumn(\"name\", trim(col(\"name\")))\n",
    "    .withColumn(\"address\", trim(col(\"address\")))\n",
    "    .withColumn(\"phone_number\", regexp_replace(trim(col(\"phone_number\")), \" \", \"\"))  # remove spaces\n",
    "    .withColumn(\"bill_amount\", regexp_replace(trim(col(\"bill_amount\")), \" \", \"\").cast(\"double\"))\n",
    "    .withColumn(\"insurance_provider\", upper(trim(col(\"insurance_provider\"))))\n",
    "    # Split full name\n",
    "    .withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0))\n",
    "    .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "    # Generate primary key and checksum\n",
    "    .withColumn(\"primary_key\", sha2(concat(col(\"patient_id\"), col(\"name\")), 256))\n",
    "    .withColumn(\"checksum_txt\", sha2(concat(\n",
    "        col(\"address\"),\n",
    "        col(\"phone_number\"),\n",
    "        col(\"bill_amount\"),\n",
    "        col(\"policy_id\"),\n",
    "        col(\"insurance_provider\")\n",
    "    ), 256))\n",
    "    # Metadata columns\n",
    "    .withColumn(\"created_at\", current_timestamp())\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n",
    "    .withColumn(\"load_ctl_key\", lit(1001))\n",
    "    .withColumn(\"is_current\", lit(\"Y\"))\n",
    "    .withColumn(\"txn_type\", lit(\"I\"))\n",
    ")\n",
    "\n",
    "# Save landing table to DBFS\n",
    "patient_landing.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/Mini_Project/landing/patient_landing\")\n",
    "patient_landing.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6666956-c78d-4dca-8d7a-be46073591e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, upper, regexp_replace, sha2, concat, current_timestamp, lit\n",
    "\n",
    "# Load insurance CSV\n",
    "insurance_raw = spark.read.csv(\"dbfs:/FileStore/Mini_Project/Source_data/Insurance_Source.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Clean and transform\n",
    "insurance_landing = (\n",
    "    insurance_raw\n",
    "    .withColumn(\"policy_id\", trim(col(\"policy_id\")))\n",
    "    .withColumn(\"insurance_provider\", upper(trim(col(\"insurance_provider\"))))\n",
    "    .withColumn(\"claim_status\", upper(trim(col(\"claim_status\"))))\n",
    "    .withColumn(\"amount_covered\", regexp_replace(trim(col(\"amount_covered\")), \" \", \"\").cast(\"double\"))\n",
    "    # Generate checksum for SCD2\n",
    "    .withColumn(\"checksum_txt\", sha2(concat(\n",
    "        col(\"insurance_provider\"),\n",
    "        col(\"claim_status\"),\n",
    "        col(\"amount_covered\")\n",
    "    ), 256))\n",
    "    # Metadata columns\n",
    "    .withColumn(\"created_at\", current_timestamp())\n",
    "    .withColumn(\"updated_at\", current_timestamp())\n",
    "    .withColumn(\"load_ctl_key\", lit(2001))\n",
    "    .withColumn(\"is_current\", lit(\"Y\"))\n",
    "    .withColumn(\"txn_type\", lit(\"I\"))\n",
    ")\n",
    "\n",
    "# Save landing table\n",
    "insurance_landing.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/Mini_Project/landing/insurance_landing\")\n",
    "insurance_landing.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ddf085d-2cd0-4300-ada1-739f15cc2547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, lit, current_timestamp\n",
    "\n",
    "# Load cleaned patient landing table\n",
    "patient_landing = spark.read.parquet(\"dbfs:/FileStore/Mini_Project/landing/patient_landing\")\n",
    "\n",
    "# Add SCD2 fields\n",
    "patient_dim = (\n",
    "    patient_landing\n",
    "    .withColumn(\"effective_start_dt\", current_timestamp())\n",
    "    .withColumn(\"effective_end_dt\", lit(\"9999-12-31\"))\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "window_patient = Window.orderBy(\"patient_id\")\n",
    "patient_dim = patient_dim.withColumn(\"patient_dim_key\", row_number().over(window_patient))\n",
    "\n",
    "# Save patient dimension\n",
    "patient_dim.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/Mini_Project/dim/patient_dim\")\n",
    "\n",
    "patient_dim.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26fe1688-6de1-40d7-af1a-55b8725e282f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load cleaned insurance landing table\n",
    "insurance_landing = spark.read.parquet(\"dbfs:/FileStore/Mini_Project/landing/insurance_landing\")\n",
    "\n",
    "# Add SCD2 fields\n",
    "insurance_dim = (\n",
    "    insurance_landing\n",
    "    .withColumn(\"effective_start_dt\", current_timestamp())\n",
    "    .withColumn(\"effective_end_dt\", lit(\"9999-12-31\"))\n",
    ")\n",
    "\n",
    "# Add surrogate key\n",
    "window_insurance = Window.orderBy(\"policy_id\")\n",
    "insurance_dim = insurance_dim.withColumn(\"insurance_dim_key\", row_number().over(window_insurance))\n",
    "\n",
    "# Save insurance dimension\n",
    "insurance_dim.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/Mini_Project/dim/insurance_dim\")\n",
    "\n",
    "insurance_dim.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9894d56e-b516-4232-bcfe-7c823b870fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# Load dim tables\n",
    "patient_dim = spark.read.parquet(\"dbfs:/FileStore/Mini_Project/dim/patient_dim\")\n",
    "insurance_dim = spark.read.parquet(\"dbfs:/FileStore/Mini_Project/dim/insurance_dim\")\n",
    "\n",
    "# Create Fact table by joining on policy_id\n",
    "fact_df = (\n",
    "    patient_dim.alias(\"p\")\n",
    "    .join(\n",
    "        insurance_dim.alias(\"i\"),\n",
    "        on=\"policy_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"p.patient_dim_key\"),\n",
    "        col(\"i.insurance_dim_key\"),\n",
    "        col(\"p.patient_id\"),\n",
    "        col(\"p.policy_id\"),\n",
    "        col(\"p.bill_amount\"),\n",
    "        col(\"i.amount_covered\"),\n",
    "        col(\"i.claim_status\"),\n",
    "        current_timestamp().alias(\"snapshot_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save fact table\n",
    "fact_df.write.mode(\"overwrite\").parquet(\"dbfs:/FileStore/Mini_Project/fact/fact_patient_insurance\")\n",
    "\n",
    "fact_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04aea618-ce01-4f8f-982f-d2b95c4dc852",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW patient_landing_view AS\n",
    "SELECT * \n",
    "FROM parquet.`dbfs:/FileStore/Mini_Project/landing/patient_landing`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe55ae0-9958-45c1-b6a4-fe2736e7ca4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Convert patient dim to Delta\n",
    "CREATE OR REPLACE TABLE patient_dim\n",
    "USING DELTA\n",
    "AS SELECT * FROM parquet.`dbfs:/FileStore/Mini_Project/dim/patient_dim`;\n",
    "\n",
    "-- Convert insurance dim to Delta\n",
    "CREATE OR REPLACE TABLE insurance_dim\n",
    "USING DELTA\n",
    "AS SELECT * FROM parquet.`dbfs:/FileStore/Mini_Project/dim/insurance_dim`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb2579bf-75c3-444c-babf-358c589e0b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMP VIEW patient_landing_final AS\n",
    "SELECT\n",
    "    src.*,\n",
    "    CASE\n",
    "        WHEN dim.patient_id IS NULL THEN 'I'                               -- New record\n",
    "        WHEN dim.checksum_txt <> src.checksum_txt THEN 'U'                 -- Updated record\n",
    "        ELSE 'N'                                                           -- No change\n",
    "    END AS transaction_ind\n",
    "FROM patient_landing_view src\n",
    "LEFT JOIN patient_dim dim\n",
    "       ON src.patient_id = dim.patient_id\n",
    "      AND dim.is_current = 'Y';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f5b551-f739-4ce2-b8f1-3221063e6000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"MERGE INTO patient_dim AS dim\n",
    "USING patient_landing_view AS src\n",
    "ON dim.patient_id = src.patient_id\n",
    "  AND dim.is_current = 'Y'  \n",
    "\n",
    "WHEN MATCHED AND dim.checksum_txt <> src.checksum_txt THEN\n",
    "  UPDATE SET\n",
    "    dim.effective_end_dt = current_timestamp(),\n",
    "    dim.is_current = 'N'\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "      patient_dim_key,\n",
    "      patient_id,\n",
    "      first_name,\n",
    "      last_name,\n",
    "      age,\n",
    "      address,\n",
    "      phone_number,\n",
    "      bill_amount,\n",
    "      insurance_provider,\n",
    "      policy_id,\n",
    "      checksum_txt,\n",
    "      load_ctl_key,\n",
    "      effective_start_dt,\n",
    "      effective_end_dt,\n",
    "      is_current\n",
    "  )\n",
    "  VALUES (\n",
    "      (SELECT COALESCE(MAX(patient_dim_key), 0) + 1 FROM patient_dim),\n",
    "      src.patient_id,\n",
    "      src.first_name,\n",
    "      src.last_name,\n",
    "      src.age,\n",
    "      src.address,\n",
    "      src.phone_number,\n",
    "      src.bill_amount,\n",
    "      src.insurance_provider,\n",
    "      src.policy_id,\n",
    "      src.checksum_txt,\n",
    "      src.load_ctl_key,\n",
    "      current_timestamp(),\n",
    "      '9999-12-31',\n",
    "      'Y'\n",
    "  );\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da3a1b89-a4f4-4144-b963-db9c008093c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5011260848097337,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Initialization_run",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
